{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192d6c99-69e0-4d7e-a73d-f478484369d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 21:57:08.602627: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-03 21:57:08.667377: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-03 21:57:08.668468: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 21:57:09.581322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import calendar\n",
    "import os.path\n",
    "\n",
    "import dask.array as da\n",
    "from dask.delayed import delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers, optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Add, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2313fef5-1297-42ae-adca-9d99c7ea0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_ds = xr.open_zarr(store='/home/jovyan/shared/data/INDIAN_OCEAN_025GRID_DAILY.zarr', consolidated=True)\n",
    "\n",
    "zarr_new = zarr_ds.sel(lat=slice(35, -5), lon=slice(45,90))\n",
    "\n",
    "all_nan_dates = np.isnan(zarr_new[\"sst\"]).all(dim=[\"lon\", \"lat\"]).compute()\n",
    "\n",
    "zarr_ds = zarr_new.sel(time=all_nan_dates == False)\n",
    "\n",
    "zarr_ds = zarr_ds.sortby('time')\n",
    "zarr_ds = zarr_ds.sel(time=slice('2005-01-01', '2022-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7767139f-579f-4abb-b547-1f370e3eb68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.delayed import delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "def preprocess_day_data(day_data):\n",
    "    day_data = da.squeeze(day_data)\n",
    "    mean_val = da.nanmean(day_data).compute()  # compute here to get scalar value\n",
    "    return day_data - mean_val\n",
    "\n",
    "def preprocess_data(zarr_ds, chunk_size=200):\n",
    "    total_len = zarr_ds['sst'].shape[0]\n",
    "    chunk_shape = (chunk_size,) + zarr_ds['sst'].shape[1:]  # Adjusted chunking\n",
    "    chunks = []\n",
    "\n",
    "    for start_idx in range(0, total_len, chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, total_len)\n",
    "        \n",
    "        # Directly slice the dask array without wrapping it with da.from_array again\n",
    "        chunk = zarr_ds['sst'][start_idx:end_idx]\n",
    "        \n",
    "        processed_chunk = chunk.map_blocks(preprocess_day_data)\n",
    "        \n",
    "        # Use da.where to replace NaNs with 0.0\n",
    "        processed_chunk = da.where(da.isnan(processed_chunk), 0.0, processed_chunk)\n",
    "        \n",
    "        chunks.append(processed_chunk)\n",
    "\n",
    "    return da.concatenate(chunks, axis=0)\n",
    "\n",
    "processed_data = preprocess_data(zarr_ds).compute()\n",
    "\n",
    "def prepare_data_from_processed(processed_data, window_size=5): \n",
    "    length = processed_data.shape[0]\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(length - window_size):\n",
    "        X.append(processed_data[i:i+window_size])\n",
    "        y.append(processed_data[i+window_size])\n",
    "\n",
    "    X, y = da.array(X), da.array(y)\n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_data_from_processed(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c454fbb2-c27e-46b3-84fa-ba1b7ab88dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_series_split(X, y, train_ratio=0.7, val_ratio=0.2):\n",
    "    total_length = X.shape[0]\n",
    "    \n",
    "    # Compute end indices for each split\n",
    "    train_end = int(total_length * train_ratio)\n",
    "    val_end = int(total_length * (train_ratio + val_ratio))\n",
    "    \n",
    "    X_train = X[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    \n",
    "    X_val = X[train_end:val_end]\n",
    "    y_val = y[train_end:val_end]\n",
    "    \n",
    "    X_test = X[val_end:]\n",
    "    y_test = y[val_end:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = time_series_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0d95e7-a192-4cd8-84a5-d13f66a88856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_size, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        if embed_size % num_heads != 0:\n",
    "            raise ValueError(f\"embedding dimension = {embed_size} should be divisible by number of heads = {num_heads}\")\n",
    "        self.projection_dim = embed_size // num_heads\n",
    "        self.query_dense = Dense(embed_size)\n",
    "        self.key_dense = Dense(embed_size)\n",
    "        self.value_dense = Dense(embed_size)\n",
    "        self.combine_heads = Dense(embed_size)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_size))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfaa7b12-ec8c-44db-8c80-2306d2b90d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Reshape, GlobalAveragePooling1D,  Conv2DTranspose\n",
    "from tensorflow.keras.layers import Masking\n",
    "\n",
    "def build_model(input_shape=(5, 149, 181, 1), num_heads=4, dropout_rate=0.1, regularizer=tf.keras.regularizers.l2(0.001)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    masked_inputs = Masking(mask_value=0.0)(inputs)\n",
    "    \n",
    "    # TimeDistributed EfficientNet\n",
    "    x = TimeDistributed(EfficientNetB0(include_top=False, weights=None, pooling='avg'))(inputs)\n",
    "\n",
    "    transformer_block = MultiHeadSelfAttention(embed_size=x.shape[-1], num_heads=num_heads)\n",
    "    x = transformer_block(x)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Decode the features\n",
    "    x = Dense(149 * 181)(x)\n",
    "    x = Reshape((149, 181, 1))(x)\n",
    "\n",
    "    # Transpose Convolution layers\n",
    "    x = Conv2DTranspose(32, (3,3), strides=(1,1), padding='same')(x)\n",
    "    x = Conv2DTranspose(1, (3,3), strides=(1,1), padding='same')(x)\n",
    "    \n",
    "    outputs = x\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = build_model(input_shape=(5, 149, 181, 1))\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a49a9703-6f19-457c-a36b-fc4b4b0fee45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 5, 149, 181, 1)   0         \n",
      "                             ]                                   \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDi  (None, 5, 1280)           4049571   \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " multi_head_self_attention_  (None, None, 1280)        6558720   \n",
      " 1 (MultiHeadSelfAttention)                                      \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 1280)              0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 26969)             34547289  \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 149, 181, 1)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 149, 181, 32)      320       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2D  (None, 149, 181, 1)       289       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45156189 (172.26 MB)\n",
      "Trainable params: 45114166 (172.10 MB)\n",
      "Non-trainable params: 42023 (164.16 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2b246f-a990-4375-9ef1-7c24fd761eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 21:59:35.343742: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2420198060 exceeds 10% of free system memory.\n",
      "2023-08-03 21:59:36.947030: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2420198060 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "141/141 [==============================] - 916s 6s/step - loss: 0.4374 - mae: 0.4374 - val_loss: 0.6311 - val_mae: 0.6311\n",
      "Epoch 2/20\n",
      "141/141 [==============================] - 921s 7s/step - loss: 0.2850 - mae: 0.2850 - val_loss: 0.5321 - val_mae: 0.5321\n",
      "Epoch 3/20\n",
      "141/141 [==============================] - 944s 7s/step - loss: 0.2639 - mae: 0.2639 - val_loss: 0.2674 - val_mae: 0.2674\n",
      "Epoch 4/20\n",
      "141/141 [==============================] - 871s 6s/step - loss: 0.2580 - mae: 0.2580 - val_loss: 0.2740 - val_mae: 0.2740\n",
      "Epoch 5/20\n",
      "141/141 [==============================] - 877s 6s/step - loss: 0.2449 - mae: 0.2449 - val_loss: 0.2509 - val_mae: 0.2509\n",
      "Epoch 6/20\n",
      "141/141 [==============================] - 907s 6s/step - loss: 0.2319 - mae: 0.2319 - val_loss: 0.2502 - val_mae: 0.2502\n",
      "Epoch 7/20\n",
      "141/141 [==============================] - 862s 6s/step - loss: 0.2299 - mae: 0.2299 - val_loss: 0.2341 - val_mae: 0.2341\n",
      "Epoch 8/20\n",
      "141/141 [==============================] - 870s 6s/step - loss: 0.2252 - mae: 0.2252 - val_loss: 0.2380 - val_mae: 0.2380\n",
      "Epoch 9/20\n",
      "141/141 [==============================] - 865s 6s/step - loss: 0.2179 - mae: 0.2179 - val_loss: 0.2586 - val_mae: 0.2586\n",
      "Epoch 10/20\n",
      "141/141 [==============================] - 869s 6s/step - loss: 0.2108 - mae: 0.2108 - val_loss: 0.2283 - val_mae: 0.2283\n",
      "Epoch 11/20\n",
      "141/141 [==============================] - 865s 6s/step - loss: 0.2029 - mae: 0.2029 - val_loss: 0.2196 - val_mae: 0.2196\n",
      "Epoch 12/20\n",
      "141/141 [==============================] - 866s 6s/step - loss: 0.2041 - mae: 0.2041 - val_loss: 0.7256 - val_mae: 0.7256\n",
      "Epoch 13/20\n",
      "141/141 [==============================] - 867s 6s/step - loss: 0.2043 - mae: 0.2043 - val_loss: 0.2247 - val_mae: 0.2247\n",
      "Epoch 14/20\n",
      "141/141 [==============================] - 861s 6s/step - loss: 0.2011 - mae: 0.2011 - val_loss: 0.2473 - val_mae: 0.2473\n",
      "Epoch 15/20\n",
      "141/141 [==============================] - 854s 6s/step - loss: 0.2040 - mae: 0.2040 - val_loss: 0.2180 - val_mae: 0.2180\n",
      "Epoch 16/20\n",
      "141/141 [==============================] - 855s 6s/step - loss: 0.1907 - mae: 0.1907 - val_loss: 0.2052 - val_mae: 0.2052\n",
      "Epoch 17/20\n",
      "141/141 [==============================] - 865s 6s/step - loss: 0.1861 - mae: 0.1861 - val_loss: 0.2173 - val_mae: 0.2173\n",
      "Epoch 18/20\n",
      "141/141 [==============================] - 859s 6s/step - loss: 0.1866 - mae: 0.1866 - val_loss: 0.2919 - val_mae: 0.2919\n",
      "Epoch 19/20\n",
      "141/141 [==============================] - 856s 6s/step - loss: 0.1833 - mae: 0.1833 - val_loss: 0.2386 - val_mae: 0.2386\n",
      "Epoch 20/20\n",
      "141/141 [==============================] - 903s 6s/step - loss: 0.1807 - mae: 0.1807 - val_loss: 0.2082 - val_mae: 0.2082\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
    "\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "\n",
    "history = model.fit(train_dataset, epochs=40, validation_data=val_dataset, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7161c36d-0567-478d-ad9e-b1b5c2e401cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vis_input_data(day_data):\n",
    "    day_data = np.squeeze(day_data)\n",
    "    mean_val = np.nanmean(day_data)\n",
    "    processed_data = day_data - mean_val\n",
    "    # Replace NaNs with 0.0\n",
    "    processed_data = np.where(np.isnan(processed_data), 0.0, processed_data)\n",
    "    return processed_data\n",
    "\n",
    "def postprocess_prediction(prediction, input_data):\n",
    "    # Find positions where the last day of input_data is 0\n",
    "    land_mask = np.load('land_mask.npy')\n",
    "    # Set those positions in the prediction to NaN\n",
    "    prediction[land_mask] = np.nan\n",
    "    \n",
    "    # Add back the historical mean\n",
    "    mean_val = np.nanmean(input_data)\n",
    "    prediction = np.where(np.isnan(prediction), np.nan, prediction + mean_val)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def predict_and_plot(date_to_predict, window_size, model, dataset, plot=True):\n",
    "    # Step 1: Select the time window\n",
    "    time_index = np.where(dataset['time'].values == np.datetime64(date_to_predict))[0][0]\n",
    "    input_data_raw = dataset['sst'][time_index-window_size:time_index].values\n",
    "    true_output_raw = dataset['sst'][time_index].values\n",
    "    print(input_data_raw.shape)\n",
    "    print(true_output_raw.shape)\n",
    "    # Preprocess the input data\n",
    "    input_data = np.array([preprocess_vis_input_data(day) for day in input_data_raw])\n",
    "    \n",
    "    # Step 2: Make prediction\n",
    "    prediction = model.predict(input_data[np.newaxis, ...])[0]\n",
    "    \n",
    "    # Postprocess the prediction\n",
    "    prediction_postprocessed = postprocess_prediction(prediction, input_data_raw)\n",
    "    print(prediction_postprocessed.shape)\n",
    "    # Step 3: Visualize\n",
    "    if plot:\n",
    "        # Determine common scale for all plots\n",
    "        input_data_raw = input_data_raw[..., np.newaxis]\n",
    "        true_output_raw = true_output_raw[np.newaxis, ..., np.newaxis]\n",
    "        prediction_postprocessed = prediction_postprocessed[np.newaxis, ...]\n",
    "        \n",
    "        all_data = np.concatenate([input_data_raw, prediction_postprocessed, true_output_raw])\n",
    "        vmin = np.nanmin(all_data)\n",
    "        vmax = np.nanmax(all_data)\n",
    "        \n",
    "        def plot_sample(sample, title=''):\n",
    "            sample_2d = np.squeeze(sample)\n",
    "            plt.imshow(sample_2d, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "            plt.title(title)\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "        # show input frames\n",
    "        for i, frame in enumerate(input_data_raw):\n",
    "            plot_sample(frame, title=f'Input Frame {i+1} ({dataset[\"time\"].values[time_index-window_size+i]})')\n",
    "        \n",
    "        # show predicted output\n",
    "        plot_sample(prediction_postprocessed, title=f'Predicted Output ({date_to_predict})')\n",
    "        \n",
    "        # show true output\n",
    "        plot_sample(true_output_raw, title=f'True Output ({date_to_predict})')\n",
    "\n",
    "    return input_data_raw, prediction_postprocessed, true_output_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73bd401c-5ecb-419e-89db-b491a1648e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mae(y_true, y_pred):\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada72e86-18d2-4ac0-bd30-5dd02a383ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_and_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m date_to_predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020-08-01\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 3\u001b[0m input_data, predicted_output, true_output \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_and_plot\u001b[49m(date_to_predict, window_size, model, zarr_ds)\n\u001b[1;32m      5\u001b[0m predicted_mae \u001b[38;5;241m=\u001b[39m compute_mae(true_output, predicted_output)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE between Predicted Output and True Output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_mae\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_and_plot' is not defined"
     ]
    }
   ],
   "source": [
    "date_to_predict = '2020-08-01'\n",
    "window_size = 5\n",
    "input_data, predicted_output, true_output = predict_and_plot(date_to_predict, window_size, model, zarr_ds)\n",
    "\n",
    "predicted_mae = compute_mae(true_output, predicted_output)\n",
    "print(f\"MAE between Predicted Output and True Output: {predicted_mae}\")\n",
    "\n",
    "last_input_frame_2d = np.squeeze(last_input_frame)\n",
    "true_output_2d = np.squeeze(true_output)\n",
    "last_frame_mae = compute_mae(true_output_2d, last_input_frame_2d)\n",
    "print(f\"MAE between Last Input Frame and True Output: {last_frame_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf201b-b7aa-472d-b72d-10c7e3b86ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
